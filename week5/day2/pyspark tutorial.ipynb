{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/25 11:31:12 WARN Utils: Your hostname, Husnus-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 172.20.10.3 instead (on interface en9)\n",
      "22/11/25 11:31:12 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/25 11:31:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .master(\"local[*]\")\\\n",
    "        .appName(\"PySpark Workshop\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'userId': 1,\n",
       " 'id': 1,\n",
       " 'title': 'sunt aut facere repellat provident occaecati excepturi optio reprehenderit',\n",
       " 'body': 'quia et suscipit\\nsuscipit recusandae consequuntur expedita et cum\\nreprehenderit molestiae ut ut quas totam\\nnostrum rerum est autem sunt rem eveniet architecto'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get(\"https://jsonplaceholder.typicode.com/posts/1\").json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# + Ensure that you have a data directory\n",
    "# + Download data into multiple json files\n",
    "# - Read files using Spark and tokenize for words\n",
    "# - Calculate top100 most popular and bottom100 least popular words into parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = spark.read.json(\"data/raw/*.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "\n",
    "tokenized = j.select(f.explode(f.split(j.body,r'\\W')).alias(\"word\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/husnusensoy/miniconda39/envs/python-training38/lib/python3.8/site-packages/pyspark/sql/dataframe.py:229: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "tokenized.registerTempTable(\"tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|      word|count(1)|\n",
      "+----------+--------+\n",
      "|        et|       5|\n",
      "| molestiae|       3|\n",
      "|        ut|       3|\n",
      "|     rerum|       2|\n",
      "|       vel|       2|\n",
      "|       aut|       2|\n",
      "|     porro|       2|\n",
      "|       qui|       2|\n",
      "|voluptatem|       2|\n",
      "|       est|       2|\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select word, count(1) n from tokens group by 1 order by 2 desc\").limit(10).coalesce(1).write.parquet(\"data/asfasd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|        word|count(1)|\n",
      "+------------+--------+\n",
      "|       totam|       1|\n",
      "|    pariatur|       1|\n",
      "|     nostrum|       1|\n",
      "|  recusandae|       1|\n",
      "|         rem|       1|\n",
      "|        sunt|       1|\n",
      "|       autem|       1|\n",
      "|     eveniet|       1|\n",
      "|        quia|       1|\n",
      "|consequuntur|       1|\n",
      "+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select word, count(1) n from tokens group by 1 order by 2\").limit(10).coalesce(1).write.parquet(\"data/asfasd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2854a322fbd609b9a8dcd5de5b5ab87f22dbcd4c479f728c8212c13390cc84c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
